{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49658892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path_to_pip_installs = \"/tmp/test_env\"\n",
    "if path_to_pip_installs not in sys.path:\n",
    "    sys.path.insert(0, path_to_pip_installs)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_dataset import HDF5ContrastDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4dc182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_as_mat(dataset, out_file, var='data_fs', compression='gzip'):\n",
    "    \"\"\"Save so that LoadDataSet() shows the image upright.\"\"\"\n",
    "\n",
    "    imgs = []\n",
    "    for i in range(len(dataset)):\n",
    "        img = dataset[i]['image']          # (1,256,256)  channel‑first\n",
    "        if isinstance(img, torch.Tensor):\n",
    "            img = img.cpu().numpy()\n",
    "        img = img.squeeze(0) .T            # <-- transpose here (W, H)\n",
    "        imgs.append(img.astype(np.float32))\n",
    "\n",
    "    imgs = np.stack(imgs, axis=0)          # (N,256,256)\n",
    "    with h5py.File(out_file, 'w') as f:\n",
    "        f.create_dataset(var, data=imgs,\n",
    "                         dtype='float32', compression=compression)\n",
    "\n",
    "    print(f'✔ Saved {len(imgs)} slices to {out_file} (will load as (N,1,256,256))')\n",
    "\n",
    "def get_patient_z_dim_combinations(overview_df_filtered, contrast_list):\n",
    "    contrast_groups = overview_df_filtered[overview_df_filtered[\"contrast\"].isin(contrast_list)].groupby('contrast').apply(\n",
    "        lambda g: set(zip(g['patient_id'], g['z_dim']))\n",
    "    )\n",
    "\n",
    "    # Step 2: Take the intersection of all sets\n",
    "    common_combinations = set.intersection(*contrast_groups)\n",
    "\n",
    "    # Step 3: Convert back to a DataFrame if needed\n",
    "    result_df = pd.DataFrame(list(common_combinations), columns=['patient_id', 'z_dim'])\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def load_dataset(base_path, contrast, image_type, cfg_transform, split=\"train\", image_dim=0):\n",
    "    # Load the synthetic dataset\n",
    "    cfg_filters = {\n",
    "        \"contrast__in\": [contrast],\n",
    "        \"non_zero\": True,\n",
    "        \"image_dim\": 0,\n",
    "        \"image_type\": image_type,\n",
    "        \"split\":split,\n",
    "    }\n",
    "    dataset = HDF5ContrastDataset(\n",
    "        hdf5_path=f\"{base_path}/data_{contrast}.h5\",\n",
    "        filter=cfg_filters,\n",
    "        transform=cfg_transform,\n",
    "        stage=\"eval\",  \n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def save_both_dataset_as_mat(dataset1, dataset2, out_file1, out_file2, var='data_fs', compression='gzip'):\n",
    "    \"\"\"Save so that LoadDataSet() shows the image upright.\"\"\"\n",
    "\n",
    "    imgs_1 = []\n",
    "    imgs_2 = []\n",
    "    already_vistied_j = set()  # To avoid duplicate processing of dataset2\n",
    "    for i in range(len(dataset1)):\n",
    "        img_1 = dataset1[i]['image']          # (1,256,256)  channel‑first\n",
    "        img_1_metadata = dataset1[i]['metadata']\n",
    "        for j in range(len(dataset2)):\n",
    "            if j in already_vistied_j:\n",
    "                continue\n",
    "            img_2_metadata = dataset2[j]['metadata']\n",
    "            if img_1_metadata['patient_id'] == img_2_metadata['patient_id'] and img_1_metadata['z_dim'] == img_2_metadata['z_dim']:\n",
    "                img_2 = dataset2[j]['image']\n",
    "                if isinstance(img_1, torch.Tensor):\n",
    "                    img_1 = img_1.cpu().numpy()\n",
    "                img_1 = img_1.squeeze(0).T  # <-- transpose here (W, H)\n",
    "                imgs_1.append(img_1.astype(np.float32))\n",
    "                if isinstance(img_2, torch.Tensor):\n",
    "                    img_2 = img_2.cpu().numpy()\n",
    "                img_2 = img_2.squeeze(0).T # <-- transpose here\n",
    "                imgs_2.append(img_2.astype(np.float32))\n",
    "                already_vistied_j.add(j)\n",
    "                break\n",
    "\n",
    "    imgs_1 = np.stack(imgs_1, axis=0) # (N,256,256)\n",
    "    with h5py.File(out_file1, 'w') as f:\n",
    "        f.create_dataset(var, data=imgs_1,\n",
    "                         dtype='float32', compression=compression)\n",
    "        \n",
    "    print(f'Saved {len(imgs_1)} slices to {out_file1} (will load as (N,1,256,256))')\n",
    "        \n",
    "    imgs_2 = np.stack(imgs_2, axis=0) # (N,256,256)\n",
    "    with h5py.File(out_file2, 'w') as f:\n",
    "        f.create_dataset(var, data=imgs_2,\n",
    "                         dtype='float32', compression=compression)\n",
    "\n",
    "    print(f'Saved {len(imgs_2)} slices to {out_file2} (will load as (N,1,256,256))')\n",
    "\n",
    "\n",
    "def create_datasets(contrast1, contrast2, image_type1, image_type2, cfg_transform, base_path, output_path):\n",
    "\n",
    "    dataset1_train = load_dataset(base_path, contrast1, image_type1, cfg_transform, split=\"train\")\n",
    "    dataset2_train = load_dataset(base_path, contrast2, image_type2, cfg_transform, split=\"train\")\n",
    "\n",
    "    dataset1_val = load_dataset(base_path, contrast1, image_type1, cfg_transform, split=\"val\")\n",
    "    dataset2_val = load_dataset(base_path, contrast2, image_type2, cfg_transform, split=\"val\")\n",
    "\n",
    "    dataset1_test = load_dataset(base_path, contrast1, image_type1, cfg_transform, split=\"test\")\n",
    "    dataset2_test = load_dataset(base_path, contrast2, image_type2, cfg_transform, split=\"test\")\n",
    "\n",
    "    out_file_1_train = f\"{output_path}/{contrast1}_{contrast2}_train.h5\"\n",
    "    out_file_2_train = f\"{output_path}/{contrast2}_train.h5\"\n",
    "\n",
    "    out_file_1_val = f\"{output_path}/{contrast1}_{contrast2}_val.h5\"\n",
    "    out_file_2_val = f\"{output_path}/{contrast2}_val.h5\"\n",
    "\n",
    "    out_file_1_test = f\"{output_path}/{contrast1}_{contrast2}_test.h5\"\n",
    "    out_file_2_test = f\"{output_path}/{contrast2}_test.h5\"\n",
    "\n",
    "    save_both_dataset_as_mat(dataset1_train, dataset2_train, out_file_1_train, out_file_2_train, var='data_fs', compression='gzip')\n",
    "    save_both_dataset_as_mat(dataset1_val, dataset2_val, out_file_1_val, out_file_2_val, var='data_fs', compression='gzip')\n",
    "    save_both_dataset_as_mat(dataset1_test, dataset2_test, out_file_1_test, out_file_2_test, var='data_fs', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b70ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded normalization stats for 6 contrasts.\n",
      "Loaded normalization stats for 6 contrasts.\n",
      "Loaded normalization stats for 6 contrasts.\n",
      "Loaded normalization stats for 6 contrasts.\n",
      "Loaded normalization stats for 6 contrasts.\n",
      "Loaded normalization stats for 6 contrasts.\n",
      "Saved 1122 slices to data/my_data_group/DIXON_Diffusion_train.h5 (will load as (N,1,256,256))\n",
      "Saved 1122 slices to data/my_data_group/Diffusion_train.h5 (will load as (N,1,256,256))\n",
      "Saved 136 slices to data/my_data_group/DIXON_Diffusion_val.h5 (will load as (N,1,256,256))\n",
      "Saved 136 slices to data/my_data_group/Diffusion_val.h5 (will load as (N,1,256,256))\n",
      "Saved 148 slices to data/my_data_group/DIXON_Diffusion_test.h5 (will load as (N,1,256,256))\n",
      "Saved 148 slices to data/my_data_group/Diffusion_test.h5 (will load as (N,1,256,256))\n"
     ]
    }
   ],
   "source": [
    "cfg_transform = {\n",
    "  \"eval\": [\n",
    "    {\n",
    "      \"GroupMinMaxNormalize\": {\n",
    "        \"stats_path\": \"/home/students/studweilc1/SynthRegGAN/data/minmax_values.json\"\n",
    "      }\n",
    "    },\n",
    "\n",
    "  ]\n",
    "}\n",
    "\n",
    "base_path = \"/home/students/studweilc1/SynthRegGAN/data\"\n",
    "output_path = \"/home/students/studweilc1/SynDiff/data/my_data_group\"\n",
    "\n",
    "contrast1 = \"DIXON\"\n",
    "contrast2 = \"BOLD\"\n",
    "image_type1 = \"W\"\n",
    "image_type2 = \"s\"\n",
    "\n",
    "create_datasets(contrast1, contrast2, image_type1, image_type2, cfg_transform, base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8b3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cornelius_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
